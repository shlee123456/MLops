#!/usr/bin/env python3
"""
Phase 3-3: Gradio vLLM Demo

vLLM ì„œë²„ì™€ ì—°ê²°ëœ Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤
"""

import os
import time
import gradio as gr
from dotenv import load_dotenv
from typing import List, Tuple

# ìƒëŒ€ ê²½ë¡œ ì„í¬íŠ¸ë¥¼ ìœ„í•œ ê²½ë¡œ ì¶”ê°€
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

try:
    from serve.vllm_client import VLLMClient
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ ì‹œ
    from vllm_client import VLLMClient

load_dotenv()


class GradioVLLMInterface:
    """Gradio vLLM ì¸í„°í˜ì´ìŠ¤"""

    def __init__(self, base_url: str = "http://localhost:8000/v1"):
        """
        Args:
            base_url: vLLM ì„œë²„ URL
        """
        self.base_url = base_url
        self.client = None
        self.available_models = []

    def initialize_client(self):
        """í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            self.client = VLLMClient(base_url=self.base_url)

            # í—¬ìŠ¤ ì²´í¬
            if not self.client.health_check():
                return False, "âš  vLLM ì„œë²„ê°€ ì‘ë‹µí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”."

            # ëª¨ë¸ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
            self.available_models = self.client.list_models()

            if not self.available_models:
                return False, "âš  ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤."

            return True, f"âœ“ vLLM ì„œë²„ ì—°ê²° ì„±ê³µ! (Models: {', '.join(self.available_models)})"

        except Exception as e:
            return False, f"âœ— ì—°ê²° ì‹¤íŒ¨: {e}"

    def chat(
        self,
        message: str,
        history: List[Tuple[str, str]],
        system_prompt: str,
        model_name: str,
        max_tokens: int,
        temperature: float,
        top_p: float,
        stream: bool
    ):
        """
        ì±„íŒ… ì‘ë‹µ ìƒì„±

        Args:
            message: ì‚¬ìš©ì ë©”ì‹œì§€
            history: ëŒ€í™” ê¸°ë¡
            system_prompt: ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
            model_name: ëª¨ë¸ ì´ë¦„
            max_tokens: ìµœëŒ€ í† í°
            temperature: ì˜¨ë„
            top_p: Top-p
            stream: ìŠ¤íŠ¸ë¦¬ë° ì—¬ë¶€
        """
        if self.client is None:
            yield "âš  í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'Connect to Server' ë²„íŠ¼ì„ ë¨¼ì € í´ë¦­í•˜ì„¸ìš”."
            return

        # ë©”ì‹œì§€ êµ¬ì„±
        messages = []

        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        # ëŒ€í™” ê¸°ë¡ ì¶”ê°€
        for user_msg, bot_msg in history:
            messages.append({"role": "user", "content": user_msg})
            messages.append({"role": "assistant", "content": bot_msg})

        # í˜„ì¬ ë©”ì‹œì§€ ì¶”ê°€
        messages.append({"role": "user", "content": message})

        # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ
        if stream:
            response_text = ""
            try:
                stream_response = self.client.chat_completion(
                    messages=messages,
                    model=model_name if model_name != "auto" else None,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    stream=True
                )

                for chunk in stream_response:
                    response_text += chunk
                    yield response_text

            except Exception as e:
                yield f"âœ— ì—ëŸ¬: {e}"

        # ì¼ë°˜ ëª¨ë“œ
        else:
            try:
                response = self.client.chat_completion(
                    messages=messages,
                    model=model_name if model_name != "auto" else None,
                    max_tokens=max_tokens,
                    temperature=temperature,
                    top_p=top_p,
                    stream=False
                )

                if "error" in response:
                    yield f"âœ— ì—ëŸ¬: {response['error']}"
                else:
                    yield response["content"]

            except Exception as e:
                yield f"âœ— ì—ëŸ¬: {e}"


def create_interface():
    """Gradio ì¸í„°í˜ì´ìŠ¤ ìƒì„±"""

    interface = GradioVLLMInterface()

    with gr.Blocks(title="vLLM Chatbot Demo", theme=gr.themes.Soft()) as demo:
        gr.Markdown(
            """
            # ğŸš€ vLLM Chatbot Demo

            ê³ ì„±ëŠ¥ vLLM ì„œë²„ì™€ ì—°ê²°ëœ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
            """
        )

        with gr.Row():
            with gr.Column(scale=3):
                # ì—°ê²° ìƒíƒœ
                with gr.Group():
                    gr.Markdown("### ì„œë²„ ì—°ê²°")
                    server_url = gr.Textbox(
                        label="vLLM Server URL",
                        value=os.getenv("VLLM_BASE_URL", "http://localhost:8000/v1"),
                        placeholder="http://localhost:8000/v1"
                    )
                    connect_btn = gr.Button("Connect to Server", variant="primary")
                    connection_status = gr.Textbox(
                        label="Connection Status",
                        value="Not connected",
                        interactive=False
                    )

                # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸
                with gr.Group():
                    gr.Markdown("### ì‹œìŠ¤í…œ ì„¤ì •")
                    system_prompt = gr.Textbox(
                        label="System Prompt",
                        value="You are a helpful AI assistant specialized in MLOps and DevOps.",
                        lines=3,
                        placeholder="ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”..."
                    )
                    model_dropdown = gr.Dropdown(
                        label="Model",
                        choices=["auto"],
                        value="auto",
                        interactive=True
                    )

            with gr.Column(scale=2):
                # ìƒì„± íŒŒë¼ë¯¸í„°
                with gr.Group():
                    gr.Markdown("### ìƒì„± íŒŒë¼ë¯¸í„°")

                    max_tokens = gr.Slider(
                        minimum=50,
                        maximum=2048,
                        value=512,
                        step=1,
                        label="Max Tokens",
                        info="ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜"
                    )

                    temperature = gr.Slider(
                        minimum=0.0,
                        maximum=2.0,
                        value=0.7,
                        step=0.1,
                        label="Temperature",
                        info="ë†’ì„ìˆ˜ë¡ ì°½ì˜ì , ë‚®ì„ìˆ˜ë¡ ê²°ì •ì "
                    )

                    top_p = gr.Slider(
                        minimum=0.0,
                        maximum=1.0,
                        value=0.9,
                        step=0.05,
                        label="Top-p",
                        info="ëˆ„ì  í™•ë¥  ì„ê³„ê°’"
                    )

                    stream_checkbox = gr.Checkbox(
                        label="Enable Streaming",
                        value=True,
                        info="ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œ (ì‹¤ì‹œê°„ ì‘ë‹µ)"
                    )

        # ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
        gr.Markdown("### ğŸ’¬ Chat")

        chatbot = gr.Chatbot(
            label="Conversation",
            height=400,
            show_copy_button=True
        )

        with gr.Row():
            msg = gr.Textbox(
                label="Your Message",
                placeholder="ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
                lines=2,
                scale=4
            )
            send_btn = gr.Button("Send", variant="primary", scale=1)

        with gr.Row():
            clear_btn = gr.Button("Clear Chat")

        # ì˜ˆì œ
        gr.Examples(
            examples=[
                ["What is MLOps and why is it important?"],
                ["Explain the difference between LoRA and QLoRA fine-tuning."],
                ["How do I set up a CI/CD pipeline for ML models?"],
                ["Write a Python function to calculate accuracy metrics."],
                ["What are the best practices for model monitoring in production?"]
            ],
            inputs=msg,
            label="Example Questions"
        )

        # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        def update_connection(url):
            """ì„œë²„ ì—°ê²°"""
            interface.base_url = url
            success, message = interface.initialize_client()

            if success:
                # ëª¨ë¸ ëª©ë¡ ì—…ë°ì´íŠ¸
                models = ["auto"] + interface.available_models
                return message, gr.Dropdown(choices=models, value="auto")
            else:
                return message, gr.Dropdown(choices=["auto"], value="auto")

        def respond(message, chat_history, system, model, max_tok, temp, top, strm):
            """ì±„íŒ… ì‘ë‹µ"""
            # ì‘ë‹µ ìƒì„±
            bot_message = interface.chat(
                message=message,
                history=chat_history,
                system_prompt=system,
                model_name=model,
                max_tokens=max_tok,
                temperature=temp,
                top_p=top,
                stream=strm
            )

            # ìŠ¤íŠ¸ë¦¬ë°ì¸ ê²½ìš°
            if strm:
                chat_history.append([message, ""])
                for response in bot_message:
                    chat_history[-1][1] = response
                    yield "", chat_history
            else:
                # ì¼ë°˜ ëª¨ë“œ
                response = next(bot_message)
                chat_history.append([message, response])
                yield "", chat_history

        # ë²„íŠ¼ ì—°ê²°
        connect_btn.click(
            fn=update_connection,
            inputs=[server_url],
            outputs=[connection_status, model_dropdown]
        )

        send_btn.click(
            fn=respond,
            inputs=[
                msg, chatbot, system_prompt, model_dropdown,
                max_tokens, temperature, top_p, stream_checkbox
            ],
            outputs=[msg, chatbot]
        )

        msg.submit(
            fn=respond,
            inputs=[
                msg, chatbot, system_prompt, model_dropdown,
                max_tokens, temperature, top_p, stream_checkbox
            ],
            outputs=[msg, chatbot]
        )

        clear_btn.click(
            fn=lambda: ([], ""),
            outputs=[chatbot, msg]
        )

        gr.Markdown(
            """
            ---
            ### ì‚¬ìš© ë°©ë²•

            1. **ì„œë²„ ì‹œì‘**: vLLM ì„œë²„ë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”
               ```bash
               python src/serve/01_vllm_server.py
               ```

            2. **ì—°ê²°**: 'Connect to Server' ë²„íŠ¼ í´ë¦­

            3. **ì±„íŒ…**: ë©”ì‹œì§€ë¥¼ ì…ë ¥í•˜ê³  Send ë²„íŠ¼ í´ë¦­

            ### íŒ
            - ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“œëŠ” ì‹¤ì‹œê°„ ì‘ë‹µì„ ì œê³µí•©ë‹ˆë‹¤
            - Temperatureë¥¼ ë‚®ì¶”ë©´ ë” ì¼ê´€ëœ ì‘ë‹µì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤
            - System Promptë¥¼ ìˆ˜ì •í•˜ì—¬ AIì˜ ì—­í• ì„ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
            """
        )

    return demo


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("\n" + "="*60)
    print("  Gradio vLLM Demo")
    print("="*60 + "\n")

    print("Starting Gradio interface...")
    print("\nMake sure vLLM server is running:")
    print("  python src/serve/01_vllm_server.py")
    print()

    demo = create_interface()

    # ì„œë²„ ì‹œì‘
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )


if __name__ == "__main__":
    main()
