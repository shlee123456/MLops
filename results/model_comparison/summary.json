{
  "LoRA": {
    "final_loss": 1.9956,
    "avg_loss_last_50": 1.870674,
    "avg_gradient_norm": 0.3698577673758491,
    "total_steps": 1188,
    "epochs": 2.0,
    "batch_size": 2
  },
  "QLoRA": {
    "final_loss": 2.0098,
    "avg_loss_last_50": 1.887456,
    "avg_gradient_norm": 0.38698788770174575,
    "total_steps": 1188,
    "epochs": 2.0,
    "batch_size": 4
  },
  "comparison": {
    "loss_difference": 0.014199999999999768,
    "loss_improvement_pct": -0.7115654439767373
  }
}